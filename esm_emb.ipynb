{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f628c4d",
   "metadata": {},
   "source": [
    "# Reproduce DDGemb method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b63206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84817ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badea\\AppData\\Local\\Temp\\ipykernel_14028\\2789855006.py:2: DtypeWarning: Columns (23,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_fireprot = pd.read_csv(path + 'fireprotdb_results.csv')\n"
     ]
    }
   ],
   "source": [
    "path = 'data/'\n",
    "df_fireprot = pd.read_csv(path + 'fireprotdb_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfbd3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>chain</th>\n",
       "      <th>position</th>\n",
       "      <th>wild_type</th>\n",
       "      <th>mutation</th>\n",
       "      <th>ddG</th>\n",
       "      <th>dTm</th>\n",
       "      <th>...</th>\n",
       "      <th>technique</th>\n",
       "      <th>technique_details</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>notes</th>\n",
       "      <th>publication_doi</th>\n",
       "      <th>publication_pubmed</th>\n",
       "      <th>hsw_job_id</th>\n",
       "      <th>datasets</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LL000001</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>245</td>\n",
       "      <td>V</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LL000002</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>95</td>\n",
       "      <td>L</td>\n",
       "      <td>V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LL000004</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>176</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LL000005</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>171</td>\n",
       "      <td>G</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LL000006</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>148</td>\n",
       "      <td>T</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment_id             protein_name uniprot_id pdb_id chain  position  \\\n",
       "0      LL000001  Haloalkane dehalogenase     P59336   1CQW     A       245   \n",
       "1      LL000002  Haloalkane dehalogenase     P59336   1CQW     A        95   \n",
       "2      LL000004  Haloalkane dehalogenase     P59336   1CQW     A       176   \n",
       "3      LL000005  Haloalkane dehalogenase     P59336   1CQW     A       171   \n",
       "4      LL000006  Haloalkane dehalogenase     P59336   1CQW     A       148   \n",
       "\n",
       "  wild_type mutation  ddG  dTm  ...  technique  technique_details  pH    tm  \\\n",
       "0         V        L  NaN  2.1  ...        NaN                NaN NaN  52.5   \n",
       "1         L        V  NaN -0.4  ...        NaN                NaN NaN  50.0   \n",
       "2         C        F  NaN  5.2  ...        NaN                NaN NaN  55.6   \n",
       "3         G        Q  NaN  3.1  ...        NaN                NaN NaN  53.5   \n",
       "4         T        L  NaN  1.1  ...        NaN                NaN NaN  51.5   \n",
       "\n",
       "   notes  publication_doi  publication_pubmed  hsw_job_id datasets  \\\n",
       "0    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "1    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "2    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "3    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "4    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "\n",
       "                                            sequence  \n",
       "0  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "1  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "2  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "3  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "4  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fireprot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d8b205d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subtilisin-chymotrypsin inhibitor-2A</td>\n",
       "      <td>11160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immunoglobulin G-binding protein G</td>\n",
       "      <td>2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tryptophan synthase alpha chain</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thermonuclease</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 kDa chaperonin</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           protein_name  count\n",
       "0  Subtilisin-chymotrypsin inhibitor-2A  11160\n",
       "1    Immunoglobulin G-binding protein G   2158\n",
       "2       Tryptophan synthase alpha chain   1915\n",
       "3                        Thermonuclease   1857\n",
       "4                     10 kDa chaperonin   1764"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_counts_1 = df_fireprot[\"protein_name\"].value_counts().reset_index()\n",
    "protein_counts_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540a919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['experiment_id', 'protein_name', 'uniprot_id', 'pdb_id', 'chain',\n",
    "       'position', 'wild_type', 'mutation', 'ddG', 'sequence']\n",
    "df_fireprot = df_fireprot[-df_fireprot['ddG'].isna()]\n",
    "df_subset = df_fireprot[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "382dc82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2IMM', '1YYX'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset[df_subset[\"protein_name\"].isna()].pdb_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7359a88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 39177 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset contains {} rows\".format(len(df_subset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc52074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence_wildtype and sequence_mutant columns\n",
    "df_fireprot.rename(columns={\"sequence\": \"sequence_wildtype\"}, inplace = True)\n",
    "df_fireprot[\"sequence_mutant\"] = df_fireprot[\"sequence_wildtype\"]\n",
    "\n",
    "# Update sequence_mutant column\n",
    "for index, row in df_fireprot.iterrows():\n",
    "    s = list(row[\"sequence_mutant\"])\n",
    "    s[row[\"position\"]-1] = row[\"mutation\"]\n",
    "    s = \"\".join(s)\n",
    "    df_fireprot.at[index,\"sequence_mutant\"] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07e44ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_fireprot[[\"sequence_wildtype\", \"sequence_mutant\"]]\n",
    "y = df_fireprot[\"ddG\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDGPredictor(nn.Module):\n",
    "    def __init__(self, \n",
    "                esm_model_name=\"facebook/esm2_t6_8M_UR50D\", \n",
    "                embedding_dim=320, \n",
    "                conv_channels=128, \n",
    "                heads=4, \n",
    "                ffn_dim=256):\n",
    "        super(DDGPredictor, self).__init__()\n",
    "        \n",
    "        # Load pretrained ESM2 model\n",
    "        self.esm_model = EsmModel.from_pretrained(esm_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "        \n",
    "        # Freeze ESM model if needed\n",
    "        for param in self.esm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 1D Conv Layer\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=embedding_dim, \n",
    "            out_channels=conv_channels, \n",
    "            kernel_size=3, \n",
    "            padding=1\n",
    "            )\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=conv_channels, \n",
    "            num_heads=heads, \n",
    "            batch_first=True)\n",
    "        \n",
    "        # Position-wise FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(conv_channels, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, conv_channels)\n",
    "        )\n",
    "        \n",
    "        # Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Dense layers for final regression\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(conv_channels * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def get_embeddings(self, seqs):\n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer(seqs, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.esm_model(**tokens)\n",
    "        # Extract per-residue embeddings\n",
    "        return outputs.last_hidden_state[:,1:-1,:]  # shape: (batch, L, 320)\n",
    "    \n",
    "    def forward(self, wt_seqs, mut_seqs):\n",
    "        # Get ESM embeddings\n",
    "        emb_wt = self.get_embeddings(wt_seqs)\n",
    "        emb_mut = self.get_embeddings(mut_seqs)\n",
    "        \n",
    "        # Take difference\n",
    "        d = emb_wt - emb_mut  # shape: (B, L, 320)\n",
    "        \n",
    "        # Conv1D expects (B, C, L)\n",
    "        c = self.conv1d(d.transpose(1, 2)).transpose(1, 2)  # shape: (B, L, conv_channels)\n",
    "        \n",
    "        # Multihead attention: q, k, v = c\n",
    "        m, _ = self.attention(c, c, c)\n",
    "        \n",
    "        # Residual connection\n",
    "        z = c + m\n",
    "        \n",
    "        # Feedforward with residual\n",
    "        p = self.ffn(z)\n",
    "        f = z + p\n",
    "        \n",
    "        # Pooling\n",
    "        f_t = f.transpose(1, 2)  # (B, C, L)\n",
    "        gp = self.global_avg_pool(f_t).squeeze(-1)  # (B, C)\n",
    "        gm = self.global_max_pool(f_t).squeeze(-1)  # (B, C)\n",
    "        \n",
    "        conc = torch.cat([gp, gm], dim=1)  # (B, 2C)\n",
    "        \n",
    "        # Final regression\n",
    "        ddg_pred = self.regressor(conc).squeeze(-1)\n",
    "        return ddg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df56bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DDGPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef2e5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define wt_seqs_batch and mut_seqs_batch, ddG_true\n",
    "class FireProtDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences.reset_index(drop=True)\n",
    "        self.targets = targets.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wt = self.sequences.loc[idx, \"sequence_wildtype\"]\n",
    "        mt = self.sequences.loc[idx, \"sequence_mutant\"]\n",
    "        ddg = self.targets[idx]\n",
    "\n",
    "        return wt, mt, torch.tensor(ddg, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff046bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FireProtDataset(x_train, y_train)\n",
    "val_dataset   = FireProtDataset(x_val, y_val)\n",
    "test_dataset  = FireProtDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "165fce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddg_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for wt_seq, mt_seq, ddg in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            ddg = ddg.to(device)\n",
    "\n",
    "            # Forward pass (model handles tokenization internally)\n",
    "            pred = model(wt_seq, mt_seq)  # wt_seq and mt_seq are lists of strings\n",
    "            loss = loss_fn(pred.squeeze(), ddg)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for wt_seq, mt_seq, ddg in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                ddg = ddg.to(device)\n",
    "                pred = model(wt_seq, mt_seq)\n",
    "                loss = loss_fn(pred.squeeze(), ddg)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddg_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
