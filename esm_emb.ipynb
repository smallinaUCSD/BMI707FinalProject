{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f628c4d",
   "metadata": {},
   "source": [
    "# Reproduce DDGemb method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090eee2",
   "metadata": {},
   "source": [
    "If you're using Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ea117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mount drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "84817ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\badea\\AppData\\Local\\Temp\\ipykernel_14028\\1709012238.py:3: DtypeWarning: Columns (23,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_fireprot = pd.read_csv(path_to_data + 'fireprotdb_results.csv')\n"
     ]
    }
   ],
   "source": [
    "#path_to_data = #path to your data on Colab\n",
    "path_to_data = 'data/'\n",
    "df_fireprot = pd.read_csv(path_to_data + 'fireprotdb_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fdfbd3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>chain</th>\n",
       "      <th>position</th>\n",
       "      <th>wild_type</th>\n",
       "      <th>mutation</th>\n",
       "      <th>ddG</th>\n",
       "      <th>dTm</th>\n",
       "      <th>...</th>\n",
       "      <th>technique</th>\n",
       "      <th>technique_details</th>\n",
       "      <th>pH</th>\n",
       "      <th>tm</th>\n",
       "      <th>notes</th>\n",
       "      <th>publication_doi</th>\n",
       "      <th>publication_pubmed</th>\n",
       "      <th>hsw_job_id</th>\n",
       "      <th>datasets</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LL000001</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>245</td>\n",
       "      <td>V</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LL000002</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>95</td>\n",
       "      <td>L</td>\n",
       "      <td>V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LL000004</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>176</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LL000005</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>171</td>\n",
       "      <td>G</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LL000006</td>\n",
       "      <td>Haloalkane dehalogenase</td>\n",
       "      <td>P59336</td>\n",
       "      <td>1CQW</td>\n",
       "      <td>A</td>\n",
       "      <td>148</td>\n",
       "      <td>T</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xfyu58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment_id             protein_name uniprot_id pdb_id chain  position  \\\n",
       "0      LL000001  Haloalkane dehalogenase     P59336   1CQW     A       245   \n",
       "1      LL000002  Haloalkane dehalogenase     P59336   1CQW     A        95   \n",
       "2      LL000004  Haloalkane dehalogenase     P59336   1CQW     A       176   \n",
       "3      LL000005  Haloalkane dehalogenase     P59336   1CQW     A       171   \n",
       "4      LL000006  Haloalkane dehalogenase     P59336   1CQW     A       148   \n",
       "\n",
       "  wild_type mutation  ddG  dTm  ...  technique  technique_details  pH    tm  \\\n",
       "0         V        L  NaN  2.1  ...        NaN                NaN NaN  52.5   \n",
       "1         L        V  NaN -0.4  ...        NaN                NaN NaN  50.0   \n",
       "2         C        F  NaN  5.2  ...        NaN                NaN NaN  55.6   \n",
       "3         G        Q  NaN  3.1  ...        NaN                NaN NaN  53.5   \n",
       "4         T        L  NaN  1.1  ...        NaN                NaN NaN  51.5   \n",
       "\n",
       "   notes  publication_doi  publication_pubmed  hsw_job_id datasets  \\\n",
       "0    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "1    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "2    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "3    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "4    NaN              NaN                 NaN      xfyu58      NaN   \n",
       "\n",
       "                                            sequence  \n",
       "0  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "1  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "2  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "3  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "4  MSEIGTGFPFDPHYVEVLGERMHYVDVGPRDGTPVLFLHGNPTSSY...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fireprot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "540a919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['experiment_id', 'protein_name', 'uniprot_id', 'pdb_id', 'chain',\n",
    "       'position', 'wild_type', 'mutation', 'ddG', 'sequence']\n",
    "df_fireprot = df_fireprot[-df_fireprot['ddG'].isna()]\n",
    "df_subset = df_fireprot[columns_to_keep].drop_duplicates(subset=['experiment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7359a88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 12131 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset contains {} rows\".format(len(df_subset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc52074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence_wildtype and sequence_mutant columns\n",
    "df_fireprot.rename(columns={\"sequence\": \"sequence_wildtype\"}, inplace = True)\n",
    "df_fireprot[\"sequence_mutant\"] = df_fireprot[\"sequence_wildtype\"]\n",
    "\n",
    "# Update sequence_mutant column\n",
    "for index, row in df_fireprot.iterrows():\n",
    "    s = list(row[\"sequence_mutant\"])\n",
    "    s[row[\"position\"]-1] = row[\"mutation\"]\n",
    "    s = \"\".join(s)\n",
    "    df_fireprot.at[index,\"sequence_mutant\"] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90249a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_fireprot[[\"sequence_wildtype\", \"sequence_mutant\"]]\n",
    "y = df_fireprot[\"ddG\"]\n",
    "X.to_csv(path_to_data + \"X.csv\", index=False)\n",
    "y.to_csv(path_to_data + \"y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759aaf7c",
   "metadata": {},
   "source": [
    "If you're reloading the data from the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(path_to_data + \"X.csv\")\n",
    "y = pd.read_csv(path_to_data + \"y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "07e44ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9a079aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDGPredictor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 esm_model_name=\"facebook/esm2_t6_8M_UR50D\",\n",
    "                 embedding_dim=320,\n",
    "                 conv_channels=128,\n",
    "                 heads=4,\n",
    "                 ffn_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device setup for trainable layers\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load pretrained ESM model on CPU\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "        self.esm_model = EsmModel.from_pretrained(esm_model_name).eval()\n",
    "        for param in self.esm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Convolutional encoder\n",
    "        self.conv1d = nn.Conv1d(embedding_dim, conv_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=conv_channels, num_heads=heads, batch_first=True)\n",
    "\n",
    "        # Position-wise feedforward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(conv_channels, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, conv_channels)\n",
    "        )\n",
    "\n",
    "        # Pooling layers\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Regression head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(conv_channels * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        # Move model parts to appropriate device\n",
    "\n",
    "    def forward(self, wt_seq, mt_seq):\n",
    "        # Tokenize and compute embeddings on CPU without gradients\n",
    "        with torch.no_grad():\n",
    "            wt_tokens = self.tokenizer(wt_seq, return_tensors=\"pt\", padding=True, truncation=True,max_length=1024)\n",
    "            mt_tokens = self.tokenizer(mt_seq, return_tensors=\"pt\", padding=True, truncation=True,max_length=1024)\n",
    "\n",
    "            wt_tokens = {k: v.to(self.device) for k, v in wt_tokens.items()}\n",
    "            mt_tokens = {k: v.to(self.device) for k, v in mt_tokens.items()}\n",
    "\n",
    "            wt_embed = self.esm_model(**wt_tokens).last_hidden_state[:, 1:-1, :]  # remove [CLS] and [SEP]\n",
    "            mt_embed = self.esm_model(**mt_tokens).last_hidden_state[:, 1:-1, :]\n",
    "\n",
    "            d = (wt_embed - mt_embed) # Move difference to GPU\n",
    "\n",
    "        # Conv1D (expects B, C, L) and transpose back\n",
    "        c = self.conv1d(d.transpose(1, 2)).transpose(1, 2)  # (B, L, C)\n",
    "\n",
    "        # Multi-head attention + residual\n",
    "        m, _ = self.attention(c, c, c)\n",
    "        z = c + m\n",
    "\n",
    "        # Position-wise feedforward + residual\n",
    "        ffn_out = self.ffn(z)\n",
    "        f = z + ffn_out\n",
    "\n",
    "        # Global pooling\n",
    "        f_t = f.transpose(1, 2)  # (B, C, L)\n",
    "        gp = self.global_avg_pool(f_t).squeeze(-1)  # (B, C)\n",
    "        gm = self.global_max_pool(f_t).squeeze(-1)  # (B, C)\n",
    "        conc = torch.cat([gp, gm], dim=1)  # (B, 2C)\n",
    "\n",
    "        # Final regression output\n",
    "        ddg_pred = self.regressor(conc).squeeze(-1)  # (B,)\n",
    "        return ddg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ef2e5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define wt_seqs_batch and mut_seqs_batch, ddG_true\n",
    "\n",
    "class FireProtDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences.reset_index(drop=True)\n",
    "        self.targets = targets.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wt = self.sequences.loc[idx, \"sequence_wildtype\"]\n",
    "        mt = self.sequences.loc[idx, \"sequence_mutant\"]\n",
    "        ddg = self.targets[idx]\n",
    "\n",
    "        return wt, mt, torch.tensor(ddg, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff046bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = FireProtDataset(x_train, y_train)\n",
    "val_dataset   = FireProtDataset(x_val, y_val)\n",
    "test_dataset  = FireProtDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4353c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddg_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    avg_train_losses = []\n",
    "    avg_val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for wt_seq, mt_seq, ddg in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            ddg = ddg.to(device)\n",
    "\n",
    "            # Forward pass (model handles tokenization internally)\n",
    "            pred = model(wt_seq, mt_seq)  # wt_seq and mt_seq are lists of strings\n",
    "            loss = loss_fn(pred.squeeze(), ddg)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # torch.cuda.empty_cache()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for wt_seq, mt_seq, ddg in tqdm.tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                ddg = ddg.to(device)\n",
    "                pred = model(wt_seq, mt_seq)\n",
    "                loss = loss_fn(pred.squeeze(), ddg)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return avg_train_losses, avg_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d5bdc5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DDGPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0080312",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_losses, avg_val_losses = train_ddg_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_model = \"model_ddg_predictor_10_epochs_lr_1e-4_batch_size_16.pth\"\n",
    "torch.save(model.state_dict(), file_path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(avg_train_losses) + 1), np.sqrt(np.array(avg_train_losses)), label='Training Loss')\n",
    "plt.plot(range(1, len(avg_val_losses) + 1), np.sqrt(np.array(avg_val_losses)), label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (RMSE)')\n",
    "plt.title('Training and Validation Loss (RMSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
